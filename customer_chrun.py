# -*- coding: utf-8 -*-
"""Kiruthiga M_CUSTOMER CHRUN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q27yb6sjccMWiHfKzPB8Va9tOKembkLz
"""

# =============================
# TELCO CUSTOMER CHURN PREDICTION (With SHAP & Feature Engineering)
# =============================

# Step 1: Install dependencies
!pip install -q tensorflow pandas scikit-learn matplotlib shap

# Step 2: Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import shap

# Step 3: Load dataset (Upload the CSV in Colab first)
df = pd.read_csv("/content/WA_Fn-UseC_-Telco-Customer-Churn.csv")
df.drop(["customerID"], axis=1, inplace=True)

# Step 4: Handle missing / invalid values
df.replace(" ", np.nan, inplace=True)
df.dropna(inplace=True)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Step 5: Feature Engineering
df['tenure_bin'] = pd.cut(df['tenure'], bins=[0, 12, 24, 48, 72], labels=[0, 1, 2, 3])
df['avg_charge'] = df['TotalCharges'] / (df['tenure'] + 1)
df['avg_charge'].fillna(df['avg_charge'].median(), inplace=True)

# Step 6: Encode categorical columns
cat_cols = df.select_dtypes(include=['object']).columns.tolist()
for c in cat_cols:
    df[c] = LabelEncoder().fit_transform(df[c].astype(str))

# Step 7: Split data
X = df.drop("Churn", axis=1)
y = df["Churn"].astype(int)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 8: Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 9: Build model
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Step 10: Train model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)

# Step 11: Evaluate model
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\n‚úÖ Test Accuracy: {acc:.4f}")

# Step 12: Plot training performance
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend(); plt.title('Accuracy')

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend(); plt.title('Loss')
plt.show()

# Step 13: SHAP Explainability (Feature Importance)
explainer = shap.Explainer(model, X_test)
shap_values = explainer(X_test)

print("\nüîç Generating SHAP summary plot (Feature Importance)...")
shap.summary_plot(shap_values, X_test, feature_names=X.columns)

# Step 14: Save model
model.save('churn_model_with_features_and_shap.h5')
print("\nüíæ Model saved as churn_model_with_features_and_shap.h5")